{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elegy Elegy is a Neural Networks framework based on Jax and Haiku. Elegy implements the Keras API but makes changes to play better with Jax & Haiku and give more flexibility around losses and metrics (more on this soon). Elegy is still in beta , feel free to test it and send us your feedback! Main Features Familiar : Elegy should feel very familiar to Keras users. Flexible : Elegy improves upon the basic Keras API by letting users optionally take more control over the definition of losses and metrics. Easy-to-use : Elegy maintains all the simplicity and ease of use that Keras brings with it. Compatible : Elegy strives to be compatible with the rest of the Jax and Haiku ecosystem. For more information take a look at the Documentation . Installation Install Elegy using pip: pip install elegy Quick Start Elegy greatly simplifies the training of Deep Learning models compared to pure Jax / Haiku where, due to Jax functional nature, users have to do a lot of book keeping around the state of the model. In Elegy just you just have to follow 3 basic steps: 1. Define the architecture inside an elegy.Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) 2. Create a Model from this module and specify additional things like losses, metrics, optimizers, and callbacks: model = elegy . Model ( module = MLP . defer (), loss = elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), aux_losses = elegy . regularizers . GlobalL2 ( l = 1e-5 ), metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . rmsprop ( 1e-3 ), ) 3. Train the model using the fit method: model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , ) And you are done! For a more information checkout: Our Getting Started tutorial. Haiku's User Manual and Documentation What is Jax? Why Jax + Haiku? Jax is a linear algebra library with the perfect recipe: * Numpy's familiar API * The speed and hardware support of XLA * Automatic Differentiation The awesome thing about Jax that Deep Learning is just a usecase that it happens to excel at but you can use it for most task you would use Numpy for. On the other hand, Haiku is a Neural Networks library built on top of Jax that implements a Module system, common Neural Network layers, and even some full architectures. Compared to other Jax-based libraries like Trax or Flax, Haiku is very minimal, polished, well documented, and makes it super easy / clean to implement Deep Learning code! We believe that Elegy can offer the best experience for coding Deep Learning applications by leveraging the power and familiarity of Jax API, the ease-of-use of Haiku's Module system, and packaging everything on top of a convenient Keras-like API. Features Model estimator class losses module metrics module regularizers module nn layers module For more information checkout the Reference API section in the Documentation . Contributing Deep Learning is evolving at an incredible rate, there is so much to do and so few hands. If you wish to contibute anything from a loss or metrics to a new awesome feature for Elegy just open an issue or send a PR! About Us License Apache","title":"Introduction"},{"location":"#elegy","text":"Elegy is a Neural Networks framework based on Jax and Haiku. Elegy implements the Keras API but makes changes to play better with Jax & Haiku and give more flexibility around losses and metrics (more on this soon). Elegy is still in beta , feel free to test it and send us your feedback!","title":"Elegy"},{"location":"#main-features","text":"Familiar : Elegy should feel very familiar to Keras users. Flexible : Elegy improves upon the basic Keras API by letting users optionally take more control over the definition of losses and metrics. Easy-to-use : Elegy maintains all the simplicity and ease of use that Keras brings with it. Compatible : Elegy strives to be compatible with the rest of the Jax and Haiku ecosystem. For more information take a look at the Documentation .","title":"Main Features"},{"location":"#installation","text":"Install Elegy using pip: pip install elegy","title":"Installation"},{"location":"#quick-start","text":"Elegy greatly simplifies the training of Deep Learning models compared to pure Jax / Haiku where, due to Jax functional nature, users have to do a lot of book keeping around the state of the model. In Elegy just you just have to follow 3 basic steps: 1. Define the architecture inside an elegy.Module : class MLP ( elegy . Module ): def call ( self , image : jnp . ndarray ) -> jnp . ndarray : mlp = hk . Sequential ([ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 10 ), ]) return mlp ( image ) 2. Create a Model from this module and specify additional things like losses, metrics, optimizers, and callbacks: model = elegy . Model ( module = MLP . defer (), loss = elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), aux_losses = elegy . regularizers . GlobalL2 ( l = 1e-5 ), metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . rmsprop ( 1e-3 ), ) 3. Train the model using the fit method: model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , ) And you are done! For a more information checkout: Our Getting Started tutorial. Haiku's User Manual and Documentation What is Jax?","title":"Quick Start"},{"location":"#why-jax-haiku","text":"Jax is a linear algebra library with the perfect recipe: * Numpy's familiar API * The speed and hardware support of XLA * Automatic Differentiation The awesome thing about Jax that Deep Learning is just a usecase that it happens to excel at but you can use it for most task you would use Numpy for. On the other hand, Haiku is a Neural Networks library built on top of Jax that implements a Module system, common Neural Network layers, and even some full architectures. Compared to other Jax-based libraries like Trax or Flax, Haiku is very minimal, polished, well documented, and makes it super easy / clean to implement Deep Learning code! We believe that Elegy can offer the best experience for coding Deep Learning applications by leveraging the power and familiarity of Jax API, the ease-of-use of Haiku's Module system, and packaging everything on top of a convenient Keras-like API.","title":"Why Jax + Haiku?"},{"location":"#features","text":"Model estimator class losses module metrics module regularizers module nn layers module For more information checkout the Reference API section in the Documentation .","title":"Features"},{"location":"#contributing","text":"Deep Learning is evolving at an incredible rate, there is so much to do and so few hands. If you wish to contibute anything from a loss or metrics to a new awesome feature for Elegy just open an issue or send a PR!","title":"Contributing"},{"location":"#about-us","text":"","title":"About Us"},{"location":"#license","text":"Apache","title":"License"},{"location":"advanced-usage/","text":"Advanced Usage Dependency Injection Manual Metrics & Losses","title":"Advanced Usage"},{"location":"advanced-usage/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"advanced-usage/#dependency-injection","text":"","title":"Dependency Injection"},{"location":"advanced-usage/#manual-metrics-losses","text":"","title":"Manual Metrics &amp; Losses"},{"location":"getting-started/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); In this tutorial we will see the explore the basic feature of Elegy . If you are a Keras user you should feel at home, if you are currently using Jax or Haiku things will appear much more streamlined. To get started you will first need to install the following dependencies: In [ ]: ! pip install elegy dataget matplotlib Note that Elegy doesn't depend on jax since there are both cpu and gpu version you can choose from so you will need to install it separately. Loading the Data \u00b6 In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [1]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website. Creating the Model \u00b6 Now that we have the data we can define our model. In Elegy you can do this by inheriting from elegy.Module and defining a call method. This method should take in some inputs, perform a series of transformation using Jax and Haiku expressions, and returns the outputs of the network. In this example we will create a simple 2 layer MLP using Haiku modules: In [3]: import jax.numpy as jnp import jax import haiku as hk import elegy class MLP ( elegy . Module ): \"\"\"Standard LeNet-300-100 MLP network.\"\"\" def __init__ ( self , n1 : int = 300 , n2 : int = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . n1 = n1 self . n2 = n2 def call ( self , image : jnp . ndarray ) -> jnp . ndarray : image = image . astype ( jnp . float32 ) / 255.0 mlp = hk . Sequential ( [ hk . Flatten (), hk . Linear ( self . n1 ), jax . nn . relu , hk . Linear ( self . n2 ), jax . nn . relu , hk . Linear ( 10 ), ] ) return mlp ( image ) Here we are using Sequential to stack two layers with relu activations and a final Linear layer with 10 units that represents the logits of the network. This code should feel familiar to most Keras / PyTorch users, the main difference here is that instead of assigning layers / modules as fields inside __init__ and later using them in call / forward , here we can just use them inplace since Haiku tracks the state for us \"behind the scenes\". Writing model code in Elegy / Haiku often feels easier since there tends to be a lot less boilerplate thanks to Haiku hooks. For a premier on Haiku please refer to this Quick Start . Note elegy.Module is just a thin wrapper over haiku.Module that adds certain Elegy-related functionalities, you can inherit from from haiku.Module instead if you wish, just remember to also rename call to __call__ Now that we have this module we can create an Elegy Model . In [8]: from jax.experimental import optix model = elegy . Model ( module = lambda : MLP ( n1 = 300 , n2 = 100 ), loss = elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), aux_losses = elegy . regularizers . GlobalL2 ( l = 1e-5 ), metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), optimizer = optix . rmsprop ( 0.0002 ), ) Much like keras.Model , an Elegy Model is tasked with performing training, evalaution, and inference. The constructor of this class accepts most of the arguments accepted by keras.Model.compile as you might have seen but there are some notable differences in this example: It requires you to pass a module as first argument It accepts additional arguments not present in Keras like aux_losses You might have also notice some weird lambda expressions around module and metrics , these arise because Haiku prohibits the creation of haiku.Module s outside of a haiku.transform . To go around this restriction we just defer instantiation of these object by wrapping them inside a lambda and calling them later. For convenience both the elegy.Module and elegy.Metric classes define a defer classmethod that which you can use to make things more readable: In [9]: model = elegy . Model ( module = MLP . defer ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True )], aux_losses = elegy . regularizers . GlobalL2 ( l = 1e-5 ), metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . rmsprop ( 0.0002 ), ) Training the Model \u00b6 Having our model instance ready we now need to pass it some data to start training. Like in Keras this is done via the fit method which contains more or less the same signature. We try to be as compatible with Keras as possible here but also remove a lot of the Tensorflow specific stuff. The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , ) ... Epoch 99/100 200/200 [==============================] - 1s 5ms/step - l2_regularization_loss: 0.0094 - loss: 0.0105 - sparse_categorical_accuracy: 0.9958 - sparse_categorical_crossentropy_loss: 0.0011 - val_l2_regularization_loss: 0.0094 - val_loss: 0.0094 - val_sparse_categorical_accuracy: 0.9813 - val_sparse_categorical_crossentropy_loss: 7.4506e-09 Epoch 100/100 200/200 [==============================] - 1s 5ms/step - l2_regularization_loss: 0.0094 - loss: 0.0271 - sparse_categorical_accuracy: 0.9966 - sparse_categorical_crossentropy_loss: 0.0177 - val_l2_regularization_loss: 0.0094 - val_loss: 0.0094 - val_sparse_categorical_accuracy: 0.9806 - val_sparse_categorical_crossentropy_loss: 4.4703e-08 We've ported Keras beloved progress bar and also implemented its Callback and History APIs. fit returns a history object which we will use next to visualize how the metrics and losses evolved during training. In [11]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history ) Doing Inference \u00b6 Having our trained model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [12]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now we can use the predict method to get their labels: In [13]: y_pred = model . predict ( x = x_sample ) Easy right? Finally lets plot the results to see if they are accurate. In [14]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Perfect! We hope you've enjoyed this tutorial. Next Steps \u00b6 Elegy is still in a very early stage, there are probably tons of bugs and missing features but we will get there. If you have some ideas or feedback on the current design we are eager to hear from you, feel free to open an issue.","title":"Getting Started"},{"location":"getting-started/#loading-the-data","text":"In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [1]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website.","title":"Loading the Data"},{"location":"getting-started/#creating-the-model","text":"Now that we have the data we can define our model. In Elegy you can do this by inheriting from elegy.Module and defining a call method. This method should take in some inputs, perform a series of transformation using Jax and Haiku expressions, and returns the outputs of the network. In this example we will create a simple 2 layer MLP using Haiku modules: In [3]: import jax.numpy as jnp import jax import haiku as hk import elegy class MLP ( elegy . Module ): \"\"\"Standard LeNet-300-100 MLP network.\"\"\" def __init__ ( self , n1 : int = 300 , n2 : int = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . n1 = n1 self . n2 = n2 def call ( self , image : jnp . ndarray ) -> jnp . ndarray : image = image . astype ( jnp . float32 ) / 255.0 mlp = hk . Sequential ( [ hk . Flatten (), hk . Linear ( self . n1 ), jax . nn . relu , hk . Linear ( self . n2 ), jax . nn . relu , hk . Linear ( 10 ), ] ) return mlp ( image ) Here we are using Sequential to stack two layers with relu activations and a final Linear layer with 10 units that represents the logits of the network. This code should feel familiar to most Keras / PyTorch users, the main difference here is that instead of assigning layers / modules as fields inside __init__ and later using them in call / forward , here we can just use them inplace since Haiku tracks the state for us \"behind the scenes\". Writing model code in Elegy / Haiku often feels easier since there tends to be a lot less boilerplate thanks to Haiku hooks. For a premier on Haiku please refer to this Quick Start . Note elegy.Module is just a thin wrapper over haiku.Module that adds certain Elegy-related functionalities, you can inherit from from haiku.Module instead if you wish, just remember to also rename call to __call__ Now that we have this module we can create an Elegy Model . In [8]: from jax.experimental import optix model = elegy . Model ( module = lambda : MLP ( n1 = 300 , n2 = 100 ), loss = elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), aux_losses = elegy . regularizers . GlobalL2 ( l = 1e-5 ), metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), optimizer = optix . rmsprop ( 0.0002 ), ) Much like keras.Model , an Elegy Model is tasked with performing training, evalaution, and inference. The constructor of this class accepts most of the arguments accepted by keras.Model.compile as you might have seen but there are some notable differences in this example: It requires you to pass a module as first argument It accepts additional arguments not present in Keras like aux_losses You might have also notice some weird lambda expressions around module and metrics , these arise because Haiku prohibits the creation of haiku.Module s outside of a haiku.transform . To go around this restriction we just defer instantiation of these object by wrapping them inside a lambda and calling them later. For convenience both the elegy.Module and elegy.Metric classes define a defer classmethod that which you can use to make things more readable: In [9]: model = elegy . Model ( module = MLP . defer ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True )], aux_losses = elegy . regularizers . GlobalL2 ( l = 1e-5 ), metrics = elegy . metrics . SparseCategoricalAccuracy . defer (), optimizer = optix . rmsprop ( 0.0002 ), )","title":"Creating the Model"},{"location":"getting-started/#training-the-model","text":"Having our model instance ready we now need to pass it some data to start training. Like in Keras this is done via the fit method which contains more or less the same signature. We try to be as compatible with Keras as possible here but also remove a lot of the Tensorflow specific stuff. The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , ) ... Epoch 99/100 200/200 [==============================] - 1s 5ms/step - l2_regularization_loss: 0.0094 - loss: 0.0105 - sparse_categorical_accuracy: 0.9958 - sparse_categorical_crossentropy_loss: 0.0011 - val_l2_regularization_loss: 0.0094 - val_loss: 0.0094 - val_sparse_categorical_accuracy: 0.9813 - val_sparse_categorical_crossentropy_loss: 7.4506e-09 Epoch 100/100 200/200 [==============================] - 1s 5ms/step - l2_regularization_loss: 0.0094 - loss: 0.0271 - sparse_categorical_accuracy: 0.9966 - sparse_categorical_crossentropy_loss: 0.0177 - val_l2_regularization_loss: 0.0094 - val_loss: 0.0094 - val_sparse_categorical_accuracy: 0.9806 - val_sparse_categorical_crossentropy_loss: 4.4703e-08 We've ported Keras beloved progress bar and also implemented its Callback and History APIs. fit returns a history object which we will use next to visualize how the metrics and losses evolved during training. In [11]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history )","title":"Training the Model"},{"location":"getting-started/#doing-inference","text":"Having our trained model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [12]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now we can use the predict method to get their labels: In [13]: y_pred = model . predict ( x = x_sample ) Easy right? Finally lets plot the results to see if they are accurate. In [14]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Perfect! We hope you've enjoyed this tutorial.","title":"Doing Inference"},{"location":"getting-started/#next-steps","text":"Elegy is still in a very early stage, there are probably tons of bugs and missing features but we will get there. If you have some ideas or feedback on the current design we are eager to hear from you, feel free to open an issue.","title":"Next Steps"},{"location":"api/Model/","text":"elegy.Model __init__ ( self , module , loss = None , loss_mode = 'match_outputs_and_labels' , aux_losses = None , metrics = None , metrics_mode = 'match_outputs_and_labels' , optimizer = None , run_eagerly = False , params = None , state = None , optimizer_state = None , metrics_state = None , initial_metrics_state = None , seed = DeviceArray ([ 0 , 42 ], dtype = uint32 )) special [summary] Parameters: Name Type Description Default module Callable [description] required loss Optional[Union[Callable, List, Dict]] [description] None loss_mode str [description]. Defaults to \"match_outputs_and_labels\". 'match_outputs_and_labels' aux_losses Optional[Callable[[], Union[List[Callable], Callable]]] [description]. Defaults to None. None metrics Optional[Union[Callable, List, Dict]] [description]. Defaults to None. None metrics_mode str [description]. Defaults to \"match_outputs_and_labels\". 'match_outputs_and_labels' optimizer Optional[jax.experimental.optix.GradientTransformation] [description]. Defaults to optix.adam(1e-3). None run_eagerly bool [description]. Defaults to False. False params Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] [description]. Defaults to None. None state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] [description]. Defaults to None. None optimizer_state Optional[NamedTuple] [description]. Defaults to None. None metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] [description]. Defaults to None. None initial_metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] [description]. Defaults to None. None seed Union[jax.numpy.lax_numpy.ndarray, int] [description]. Defaults to jax.random.PRNGKey(42). DeviceArray([ 0, 42], dtype=uint32) Exceptions: Type Description ValueError [description] Source code in elegy/model.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def __init__ ( self , module : tp . Callable , loss : tp . Union [ tp . Callable , tp . List , tp . Dict , None ] = None , loss_mode : str = \"match_outputs_and_labels\" , aux_losses : tp . Optional [ tp . Callable [[], tp . Union [ tp . List [ tp . Callable ], tp . Callable ]] ] = None , metrics : tp . Union [ tp . Callable , tp . List , tp . Dict , None ] = None , metrics_mode : str = \"match_outputs_and_labels\" , optimizer : tp . Optional [ optix . GradientTransformation ] = None , run_eagerly : bool = False , params : tp . Optional [ hk . Params ] = None , state : tp . Optional [ hk . State ] = None , optimizer_state : tp . Optional [ optix . OptState ] = None , metrics_state : tp . Optional [ hk . State ] = None , initial_metrics_state : tp . Optional [ hk . State ] = None , seed : tp . Union [ jnp . ndarray , int ] = jax . random . PRNGKey ( 42 ), ): \"\"\"[summary] Args: module (tp.Optional[tp.Callable]): [description] loss (tp.Callable): [description] loss_mode (str, optional): [description]. Defaults to \"match_outputs_and_labels\". aux_losses (tp.Optional[ tp.Callable[[], tp.Union[tp.List[tp.Callable], tp.Callable]] ], optional): [description]. Defaults to None. metrics (tp.Optional[tp.Callable], optional): [description]. Defaults to None. metrics_mode (str, optional): [description]. Defaults to \"match_outputs_and_labels\". optimizer (optix.GradientTransformation, optional): [description]. Defaults to optix.adam(1e-3). run_eagerly (bool, optional): [description]. Defaults to False. params (tp.Optional[hk.Params], optional): [description]. Defaults to None. state (tp.Optional[hk.State], optional): [description]. Defaults to None. optimizer_state (tp.Optional[optix.OptState], optional): [description]. Defaults to None. metrics_state (tp.Optional[hk.State], optional): [description]. Defaults to None. initial_metrics_state (tp.Optional[hk.State], optional): [description]. Defaults to None. seed (tp.Union[jnp.ndarray, int], optional): [description]. Defaults to jax.random.PRNGKey(42). Raises: ValueError: [description] \"\"\" if metrics is not None : metrics = metric_modes . get_mode_function ( metrics_mode )( metrics ) if loss is None : def loss_ ( y_true , y_pred ): return 0.0 loss = loss_ loss = loss_modes . get_mode_function ( loss_mode )( loss ) def model_fn ( * args , ** kwargs ): module = self . _module_fn () return utils . inject_dependencies ( module )( * args , ** kwargs ) self . _module_fn = module self . _model_transform = hk . transform_with_state ( model_fn ) self . _loss_fn = utils . inject_dependencies ( loss ) self . _aux_losses = ( loss_modes . get_aux_losses_fn ( aux_losses ) if aux_losses is not None else None ) self . _metrics_transform = ( hk . transform_with_state ( utils . inject_dependencies ( metrics , rename = { \"__params\" : \"params\" }) ) if metrics else None ) self . _optimizer = optimizer if optimizer is not None else optix . adam ( 1e-3 ) self . _rngs = hk . PRNGSequence ( seed ) self . _params = params self . _state = state self . _optimizer_state = optimizer_state self . _metrics_state = metrics_state self . _initial_metrics_state = initial_metrics_state self . run_eagerly = run_eagerly evaluate ( self , x , y = None , verbose = 1 , batch_size = None , sample_weight = None , steps = None , callbacks = None ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Tuple[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Iterable] Input data. It could be: - A Numpy array (or array-like), or a list required y Optional[Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Tuple[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]]]] Target data. Like the input data x , it could be either Numpy None batch_size Optional[int] Integer or None . Number of samples per gradient update. If None verbose int 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. 1 sample_weight Optional[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]] Optional Numpy array of weights for the test samples, None Numpy array with the same length as the input samples (1 1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . required steps Optional[int] Integer or None . Total number of steps (batches of samples) None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of keras.callbacks.Callback instances. List of None max_queue_size Integer. Used for generator or keras.utils.Sequence required workers Integer. Used for generator or keras.utils.Sequence input required use_multiprocessing Boolean. Used for generator or required return_dict If True , loss and metric results are returned as a dict, required See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Type Description Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model.py 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 def evaluate ( self , x : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , tp . Union [ np . ndarray , jnp . ndarray ]], tp . Tuple [ tp . Union [ np . ndarray , jnp . ndarray ]], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , tp . Union [ np . ndarray , jnp . ndarray ]], tp . Tuple [ tp . Union [ np . ndarray , jnp . ndarray ]], None , ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ tp . Union [ np . ndarray , jnp . ndarray ]] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ): \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. In this case you should make sure to specify `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: ValueError: in case of invalid arguments. \"\"\" data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) sample_weight = batch [ 2 ] if len ( batch ) == 3 else None tmp_logs = self . test_on_batch ( x = batch [ 0 ], y = batch [ 1 ], sample_weight = sample_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs fit ( self , x , y = None , batch_size = None , epochs = 1 , verbose = 1 , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 ) Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Tuple[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Iterable] Input data. It could be: required y Optional[Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Tuple[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]]]] Target data. Like the input data x , None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of keras.callbacks.Callback instances. List of callbacks to apply during training. See elegy.callbacks . None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or 0.0 validation_data Optional[Union[Tuple, Iterable]] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset For the first two cases, batch_size must be provided. For the last case, validation_steps could be provided. Note that validation_data does not support all the data types that are supported in x , eg, dict, generator or keras.utils.Sequence . None shuffle bool Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]] Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, generator, or None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. required workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. required use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. required Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or elegy.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description RuntimeError If the model was never compiled. ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model.py 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 def fit ( self , x : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , tp . Union [ np . ndarray , jnp . ndarray ]], tp . Tuple [ tp . Union [ np . ndarray , jnp . ndarray ]], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , tp . Union [ np . ndarray , jnp . ndarray ]], tp . Tuple [ tp . Union [ np . ndarray , jnp . ndarray ]], None , ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ tp . Union [ np . ndarray , jnp . ndarray ]] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , ): \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `elegy.callbacks`. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy arrays or tensors - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays - dataset For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` could be provided. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict, generator or `keras.utils.Sequence`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. In this case you should make sure to specify `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or elegy.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: If the model was never compiled. ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) sample_weight = batch [ 2 ] if len ( batch ) == 3 else None tmp_logs = self . train_on_batch ( x = batch [ 0 ], y = batch [ 1 ], sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) epoch_logs = copy . copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) # print( # f\"epoch: {epoch} - \" # + \" - \".join(f\"{key}: {value:.3f}\" for key, value in epoch_logs.items()) # ) if self . stop_training : break callbacks . on_train_end () return self . history predict ( self , x , verbose = 0 , batch_size = None , steps = None , callbacks = None ) Generates output predictions for the input samples. Computation is done in batches. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using __call__ is recommended for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behaves differently during inference. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Tuple[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Iterable] Input samples. It could be: required batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict will run until the input dataset is exhausted. None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . None max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. required workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. required use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. required See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 def predict ( self , x : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , tp . Union [ np . ndarray , jnp . ndarray ]], tp . Tuple [ tp . Union [ np . ndarray , jnp . ndarray ]], tp . Iterable , ], verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ): \"\"\"Generates output predictions for the input samples. Computation is done in batches. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using `__call__` is recommended for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behaves differently during inference. Arguments: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs ,) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs","title":"Model"},{"location":"api/Model/#elegymodel","text":"","title":"elegy.Model"},{"location":"api/Model/#elegy.model.Model","text":"","title":"elegy.model.Model"},{"location":"api/Model/#elegy.model.Model.__init__","text":"[summary] Parameters: Name Type Description Default module Callable [description] required loss Optional[Union[Callable, List, Dict]] [description] None loss_mode str [description]. Defaults to \"match_outputs_and_labels\". 'match_outputs_and_labels' aux_losses Optional[Callable[[], Union[List[Callable], Callable]]] [description]. Defaults to None. None metrics Optional[Union[Callable, List, Dict]] [description]. Defaults to None. None metrics_mode str [description]. Defaults to \"match_outputs_and_labels\". 'match_outputs_and_labels' optimizer Optional[jax.experimental.optix.GradientTransformation] [description]. Defaults to optix.adam(1e-3). None run_eagerly bool [description]. Defaults to False. False params Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] [description]. Defaults to None. None state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] [description]. Defaults to None. None optimizer_state Optional[NamedTuple] [description]. Defaults to None. None metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] [description]. Defaults to None. None initial_metrics_state Optional[Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]]] [description]. Defaults to None. None seed Union[jax.numpy.lax_numpy.ndarray, int] [description]. Defaults to jax.random.PRNGKey(42). DeviceArray([ 0, 42], dtype=uint32) Exceptions: Type Description ValueError [description] Source code in elegy/model.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def __init__ ( self , module : tp . Callable , loss : tp . Union [ tp . Callable , tp . List , tp . Dict , None ] = None , loss_mode : str = \"match_outputs_and_labels\" , aux_losses : tp . Optional [ tp . Callable [[], tp . Union [ tp . List [ tp . Callable ], tp . Callable ]] ] = None , metrics : tp . Union [ tp . Callable , tp . List , tp . Dict , None ] = None , metrics_mode : str = \"match_outputs_and_labels\" , optimizer : tp . Optional [ optix . GradientTransformation ] = None , run_eagerly : bool = False , params : tp . Optional [ hk . Params ] = None , state : tp . Optional [ hk . State ] = None , optimizer_state : tp . Optional [ optix . OptState ] = None , metrics_state : tp . Optional [ hk . State ] = None , initial_metrics_state : tp . Optional [ hk . State ] = None , seed : tp . Union [ jnp . ndarray , int ] = jax . random . PRNGKey ( 42 ), ): \"\"\"[summary] Args: module (tp.Optional[tp.Callable]): [description] loss (tp.Callable): [description] loss_mode (str, optional): [description]. Defaults to \"match_outputs_and_labels\". aux_losses (tp.Optional[ tp.Callable[[], tp.Union[tp.List[tp.Callable], tp.Callable]] ], optional): [description]. Defaults to None. metrics (tp.Optional[tp.Callable], optional): [description]. Defaults to None. metrics_mode (str, optional): [description]. Defaults to \"match_outputs_and_labels\". optimizer (optix.GradientTransformation, optional): [description]. Defaults to optix.adam(1e-3). run_eagerly (bool, optional): [description]. Defaults to False. params (tp.Optional[hk.Params], optional): [description]. Defaults to None. state (tp.Optional[hk.State], optional): [description]. Defaults to None. optimizer_state (tp.Optional[optix.OptState], optional): [description]. Defaults to None. metrics_state (tp.Optional[hk.State], optional): [description]. Defaults to None. initial_metrics_state (tp.Optional[hk.State], optional): [description]. Defaults to None. seed (tp.Union[jnp.ndarray, int], optional): [description]. Defaults to jax.random.PRNGKey(42). Raises: ValueError: [description] \"\"\" if metrics is not None : metrics = metric_modes . get_mode_function ( metrics_mode )( metrics ) if loss is None : def loss_ ( y_true , y_pred ): return 0.0 loss = loss_ loss = loss_modes . get_mode_function ( loss_mode )( loss ) def model_fn ( * args , ** kwargs ): module = self . _module_fn () return utils . inject_dependencies ( module )( * args , ** kwargs ) self . _module_fn = module self . _model_transform = hk . transform_with_state ( model_fn ) self . _loss_fn = utils . inject_dependencies ( loss ) self . _aux_losses = ( loss_modes . get_aux_losses_fn ( aux_losses ) if aux_losses is not None else None ) self . _metrics_transform = ( hk . transform_with_state ( utils . inject_dependencies ( metrics , rename = { \"__params\" : \"params\" }) ) if metrics else None ) self . _optimizer = optimizer if optimizer is not None else optix . adam ( 1e-3 ) self . _rngs = hk . PRNGSequence ( seed ) self . _params = params self . _state = state self . _optimizer_state = optimizer_state self . _metrics_state = metrics_state self . _initial_metrics_state = initial_metrics_state self . run_eagerly = run_eagerly","title":"__init__()"},{"location":"api/Model/#elegy.model.Model.evaluate","text":"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Tuple[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Iterable] Input data. It could be: - A Numpy array (or array-like), or a list required y Optional[Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Tuple[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]]]] Target data. Like the input data x , it could be either Numpy None batch_size Optional[int] Integer or None . Number of samples per gradient update. If None verbose int 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. 1 sample_weight Optional[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]] Optional Numpy array of weights for the test samples, None Numpy array with the same length as the input samples (1 1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . required steps Optional[int] Integer or None . Total number of steps (batches of samples) None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of keras.callbacks.Callback instances. List of None max_queue_size Integer. Used for generator or keras.utils.Sequence required workers Integer. Used for generator or keras.utils.Sequence input required use_multiprocessing Boolean. Used for generator or required return_dict If True , loss and metric results are returned as a dict, required See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Type Description Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model.py 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 def evaluate ( self , x : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , tp . Union [ np . ndarray , jnp . ndarray ]], tp . Tuple [ tp . Union [ np . ndarray , jnp . ndarray ]], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , tp . Union [ np . ndarray , jnp . ndarray ]], tp . Tuple [ tp . Union [ np . ndarray , jnp . ndarray ]], None , ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ tp . Union [ np . ndarray , jnp . ndarray ]] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ): \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. In this case you should make sure to specify `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: ValueError: in case of invalid arguments. \"\"\" data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) sample_weight = batch [ 2 ] if len ( batch ) == 3 else None tmp_logs = self . test_on_batch ( x = batch [ 0 ], y = batch [ 1 ], sample_weight = sample_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs","title":"evaluate()"},{"location":"api/Model/#elegy.model.Model.fit","text":"Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Tuple[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Iterable] Input data. It could be: required y Optional[Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Tuple[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]]]] Target data. Like the input data x , None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of keras.callbacks.Callback instances. List of callbacks to apply during training. See elegy.callbacks . None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or 0.0 validation_data Optional[Union[Tuple, Iterable]] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset For the first two cases, batch_size must be provided. For the last case, validation_steps could be provided. Note that validation_data does not support all the data types that are supported in x , eg, dict, generator or keras.utils.Sequence . None shuffle bool Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]] Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, generator, or None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. required workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. required use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. required Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or elegy.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description RuntimeError If the model was never compiled. ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model.py 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 def fit ( self , x : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , tp . Union [ np . ndarray , jnp . ndarray ]], tp . Tuple [ tp . Union [ np . ndarray , jnp . ndarray ]], tp . Iterable , ], y : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , tp . Union [ np . ndarray , jnp . ndarray ]], tp . Tuple [ tp . Union [ np . ndarray , jnp . ndarray ]], None , ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ tp . Union [ np . ndarray , jnp . ndarray ]] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , ): \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during training. See `elegy.callbacks`. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or `keras.utils.Sequence` instance. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy arrays or tensors - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays - dataset For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` could be provided. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict, generator or `keras.utils.Sequence`. shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when `x` is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. In this case you should make sure to specify `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not supported when `x` is a dataset, generator, or `keras.utils.Sequence` instance, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a `tf.data` dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a `tf.data` dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or elegy.utils.Sequence to the `x` argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: If the model was never compiled. ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) sample_weight = batch [ 2 ] if len ( batch ) == 3 else None tmp_logs = self . train_on_batch ( x = batch [ 0 ], y = batch [ 1 ], sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) epoch_logs = copy . copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if validation_data and self . _should_eval ( epoch , validation_freq ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) callbacks . on_epoch_end ( epoch , epoch_logs ) # print( # f\"epoch: {epoch} - \" # + \" - \".join(f\"{key}: {value:.3f}\" for key, value in epoch_logs.items()) # ) if self . stop_training : break callbacks . on_train_end () return self . history","title":"fit()"},{"location":"api/Model/#elegy.model.Model.predict","text":"Generates output predictions for the input samples. Computation is done in batches. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using __call__ is recommended for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behaves differently during inference. Parameters: Name Type Description Default x Union[jax.numpy.lax_numpy.ndarray, numpy.ndarray, Mapping[str, Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Tuple[Union[numpy.ndarray, jax.numpy.lax_numpy.ndarray]], Iterable] Input samples. It could be: required batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict will run until the input dataset is exhausted. None callbacks Optional[Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList]] List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . None max_queue_size Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. required workers Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. required use_multiprocessing Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. required See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model.py 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 def predict ( self , x : tp . Union [ jnp . ndarray , np . ndarray , tp . Mapping [ str , tp . Union [ np . ndarray , jnp . ndarray ]], tp . Tuple [ tp . Union [ np . ndarray , jnp . ndarray ]], tp . Iterable , ], verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , ): \"\"\"Generates output predictions for the input samples. Computation is done in batches. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using `__call__` is recommended for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `tf.keras.layers.BatchNormalization` that behaves differently during inference. Arguments: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A `tf.data` dataset. - A generator or `keras.utils.Sequence` instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or `keras.utils.Sequence` instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If x is a `tf.data` dataset and `steps` is None, `predict` will run until the input dataset is exhausted. callbacks: List of `keras.callbacks.Callback` instances. List of callbacks to apply during prediction. See [callbacks](/api_docs/python/tf/keras/callbacks). max_queue_size: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum size for the generator queue. If unspecified, `max_queue_size` will default to 10. workers: Integer. Used for generator or `keras.utils.Sequence` input only. Maximum number of processes to spin up when using process-based threading. If unspecified, `workers` will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or `keras.utils.Sequence` input only. If `True`, use process-based threading. If unspecified, `use_multiprocessing` will default to `False`. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of `Unpacking behavior for iterator-like inputs` for `Model.fit`. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs ,) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs","title":"predict()"},{"location":"api/Overview/","text":"Overview Model metrics losses regularizers","title":"Overview"},{"location":"api/Overview/#overview","text":"Model metrics losses regularizers","title":"Overview"},{"location":"api/losses/CategoricalCrossentropy/","text":"elegy.losses.CategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy () assert cce ( y_true , y_pred ) == 1.177 # Calling with 'sample_weight'. assert cce ( y_true , y_pred , sample_weight = tf . constant ([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) assert cce ( y_true , y_pred ) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) assert list ( cce ( y_true , y_pred )) == [ 0.0513 , 2.303 ] Usage with the compile API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , name = None , weight = None ) special Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None name Optional[str] Optional name for the op. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/categorical_crossentropy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing call ( self , y_true , y_pred , sample_weight = None ) Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegylossescategoricalcrossentropy","text":"","title":"elegy.losses.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy () assert cce ( y_true , y_pred ) == 1.177 # Calling with 'sample_weight'. assert cce ( y_true , y_pred , sample_weight = tf . constant ([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) assert cce ( y_true , y_pred ) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) assert list ( cce ( y_true , y_pred )) == [ 0.0513 , 2.303 ] Usage with the compile API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), )","title":"elegy.losses.categorical_crossentropy.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.__init__","text":"Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None name Optional[str] Optional name for the op. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/categorical_crossentropy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.call","text":"Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"call()"},{"location":"api/losses/MeanSquaredError/","text":"elegy.losses.MeanSquaredError Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError () assert mse ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mse ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . SUM ) assert mse ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mse ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . MeanSquaredError ()], metrics = lambda : [ elegy . metrics . Mean ()], ) __init__ ( self , reduction = None , name = None , weight = None ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None name Optional[str] Optional name for the loss. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/mean_squared_error.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. name: Optional name for the loss. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , name = name , weight = weight ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegylossesmeansquarederror","text":"","title":"elegy.losses.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError","text":"Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError () assert mse ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mse ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . SUM ) assert mse ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mse ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . MeanSquaredError ()], metrics = lambda : [ elegy . metrics . Mean ()], )","title":"elegy.losses.mean_squared_error.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None name Optional[str] Optional name for the loss. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/mean_squared_error.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. name: Optional name for the loss. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , name = name , weight = weight )","title":"__init__()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/Overview/","text":"","title":"Overview"},{"location":"api/losses/SparseCategoricalCrossentropy/","text":"elegy.losses.SparseCategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array ([ 1 , 2 ]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy () result = scce ( y_true , y_pred ) # 1.177 assert jnp . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( y_true , y_pred , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = scce ( y_true , y_pred ) # 2.354 assert jnp . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = scce ( y_true , y_pred ) # [0.0513, 2.303] assert jnp . all ( jnp . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the compile API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , name = None , weight = None ) special Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. None name Optional[str] Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/sparse_categorical_crossentropy.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing call ( self , y_true , y_pred , sample_weight = None ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegylossessparsecategoricalcrossentropy","text":"","title":"elegy.losses.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array ([ 1 , 2 ]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy () result = scce ( y_true , y_pred ) # 1.177 assert jnp . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( y_true , y_pred , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = scce ( y_true , y_pred ) # 2.354 assert jnp . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = scce ( y_true , y_pred ) # [0.0513, 2.303] assert jnp . all ( jnp . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the compile API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), )","title":"elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. None name Optional[str] Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/sparse_categorical_crossentropy.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"call()"},{"location":"api/losses/mean_squared_error/","text":"elegy.losses.mean_squared_error Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegylossesmean_squared_error","text":"","title":"elegy.losses.mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegy.losses.mean_squared_error.mean_squared_error","text":"Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_squared_error.mean_squared_error"},{"location":"api/metrics/Accuracy/","text":"elegy.metrics.Accuracy Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy () result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , name = None , dtype = None ) special Creates a Accuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/accuracy.py 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `Accuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"Accuracy"},{"location":"api/metrics/Accuracy/#elegymetricsaccuracy","text":"","title":"elegy.metrics.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy","text":"Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy () result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.accuracy.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.__init__","text":"Creates a Accuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/accuracy.py 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `Accuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/CategoricalAccuracy/","text":"elegy.metrics.CategoricalAccuracy Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy () result = accuracy ( y_true = jnp . array ([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . CategoricalAccuracy ()], optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , name = None , dtype = None ) special Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/categorical_accuracy.py 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegymetricscategoricalaccuracy","text":"","title":"elegy.metrics.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy","text":"Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy () result = accuracy ( y_true = jnp . array ([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . CategoricalAccuracy ()], optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.categorical_accuracy.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.__init__","text":"Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/categorical_accuracy.py 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/Mean/","text":"elegy.metrics.Mean Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . MeanSquaredError ()], metrics = lambda : [ elegy . metrics . Mean ()], ) __init__ ( self , name = None , dtype = None ) special Creates a Mean instance. Parameters: Name Type Description Default name Optional[str] (Optional) string name of the metric instance. None dtype Optional[numpy.dtype] (Optional) data type of the metric result. None Source code in elegy/metrics/mean.py 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. \"\"\" super () . __init__ ( reduction = reduce . Reduction . WEIGHTED_MEAN , name = name , dtype = dtype ) call ( self , values , sample_weight = None ) Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"Mean"},{"location":"api/metrics/Mean/#elegymetricsmean","text":"","title":"elegy.metrics.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean","text":"Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . MeanSquaredError ()], metrics = lambda : [ elegy . metrics . Mean ()], )","title":"elegy.metrics.mean.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.__init__","text":"Creates a Mean instance. Parameters: Name Type Description Default name Optional[str] (Optional) string name of the metric instance. None dtype Optional[numpy.dtype] (Optional) data type of the metric result. None Source code in elegy/metrics/mean.py 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. \"\"\" super () . __init__ ( reduction = reduce . Reduction . WEIGHTED_MEAN , name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.call","text":"Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"call()"},{"location":"api/metrics/MeanSquaredError/","text":"elegy.metrics.MeanSquaredError Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError () result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . MeanSquaredError ()], ) __init__ ( self , name = None , dtype = None ) special Creates a MeanSquaredError instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/mean_squared_error.py 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegymetricsmeansquarederror","text":"","title":"elegy.metrics.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError","text":"Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError () result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . MeanSquaredError ()], )","title":"elegy.metrics.mean_squared_error.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.__init__","text":"Creates a MeanSquaredError instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/mean_squared_error.py 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"call()"},{"location":"api/metrics/Overview/","text":"","title":"Overview"},{"location":"api/metrics/SparseCategoricalAccuracy/","text":"elegy.metrics.SparseCategoricalAccuracy Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy () result = accuracy ( y_true = jnp . array ([ 2 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([ 1 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . SparseCategoricalAccuracy ()], optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , name = None , dtype = None ) special Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/sparse_categorical_accuracy.py 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegymetricssparsecategoricalaccuracy","text":"","title":"elegy.metrics.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy","text":"Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy () result = accuracy ( y_true = jnp . array ([ 2 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([ 1 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . SparseCategoricalAccuracy ()], optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.__init__","text":"Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/sparse_categorical_accuracy.py 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/regularizers/GlobalL1/","text":"elegy.regularizers.GlobalL1 Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model ( model_fn = model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . regularizers . GlobalL1 ( l = 1e-5 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def GlobalL1 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( model_fn=model_fn, loss=lambda: [elegy.losses.SparseCategoricalCrossentropy()], aux_losses=lambda: [elegy.regularizers.GlobalL1(l=1e-5)], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l1 = l , reduction = reduction , name = name )","title":"GlobalL1"},{"location":"api/regularizers/GlobalL1/#elegyregularizersgloball1","text":"","title":"elegy.regularizers.GlobalL1"},{"location":"api/regularizers/GlobalL1/#elegy.regularizers.global_l1.GlobalL1","text":"Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model ( model_fn = model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . regularizers . GlobalL1 ( l = 1e-5 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def GlobalL1 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( model_fn=model_fn, loss=lambda: [elegy.losses.SparseCategoricalCrossentropy()], aux_losses=lambda: [elegy.regularizers.GlobalL1(l=1e-5)], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l1 = l , reduction = reduction , name = name )","title":"elegy.regularizers.global_l1.GlobalL1"},{"location":"api/regularizers/GlobalL1L2/","text":"elegy.regularizers.GlobalL1L2 A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( model_fn = model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . regularizers . GlobalL1L2 ( l1 = 1e-5 , l2 = 1e-4 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor. call ( self , params ) Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default params Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]] A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def call ( self , params : hk . Params ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( params ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( params ) ) return regularization","title":"GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegyregularizersgloball1l2","text":"","title":"elegy.regularizers.GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2","text":"A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( model_fn = model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . regularizers . GlobalL1L2 ( l1 = 1e-5 , l2 = 1e-4 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor.","title":"elegy.regularizers.global_l1l2.GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.call","text":"Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default params Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]] A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def call ( self , params : hk . Params ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( params ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( params ) ) return regularization","title":"call()"},{"location":"api/regularizers/GlobalL2/","text":"elegy.regularizers.GlobalL2 Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . losses . GlobaL2Regularization ( l = 1e-4 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def GlobalL2 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( model_fn, loss=lambda: [elegy.losses.SparseCategoricalCrossentropy()], aux_losses=lambda: [elegy.losses.GlobaL2Regularization(l=1e-4)], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l2 = l , reduction = reduction , name = name )","title":"GlobalL2"},{"location":"api/regularizers/GlobalL2/#elegyregularizersgloball2","text":"","title":"elegy.regularizers.GlobalL2"},{"location":"api/regularizers/GlobalL2/#elegy.regularizers.global_l2.GlobalL2","text":"Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . losses . GlobaL2Regularization ( l = 1e-4 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def GlobalL2 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( model_fn, loss=lambda: [elegy.losses.SparseCategoricalCrossentropy()], aux_losses=lambda: [elegy.losses.GlobaL2Regularization(l=1e-4)], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l2 = l , reduction = reduction , name = name )","title":"elegy.regularizers.global_l2.GlobalL2"},{"location":"api/regularizers/Overview/","text":"","title":"Overview"}]}