{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elegy Elegy is a Neural Networks framework based on Jax and Haiku. Elegy can be seen as an opinionated Keras port since it keeps most of the API and documentation, but makes changes to either play better with Jax/Haiku or give more flexibility to researchers and developers, see Differences with Keras . Main Features Familiar : Elegy should feel very familiar to Keras users. Flexible : Elegy improves upon the basic Keras API by letting users optionally take more control over the definition of losses and metrics. Easy-to-use : Elegy maintains all the simplicity and ease of use that Keras brings with it. Compatible : Elegy strives to be compatible with the rest of the Jax and Haiku ecosystem. For more information take a look at the Documentation . Installation Install Elegy using pip: pip install elegy","title":"Introduction"},{"location":"#elegy","text":"Elegy is a Neural Networks framework based on Jax and Haiku. Elegy can be seen as an opinionated Keras port since it keeps most of the API and documentation, but makes changes to either play better with Jax/Haiku or give more flexibility to researchers and developers, see Differences with Keras .","title":"Elegy"},{"location":"#main-features","text":"Familiar : Elegy should feel very familiar to Keras users. Flexible : Elegy improves upon the basic Keras API by letting users optionally take more control over the definition of losses and metrics. Easy-to-use : Elegy maintains all the simplicity and ease of use that Keras brings with it. Compatible : Elegy strives to be compatible with the rest of the Jax and Haiku ecosystem. For more information take a look at the Documentation .","title":"Main Features"},{"location":"#installation","text":"Install Elegy using pip: pip install elegy","title":"Installation"},{"location":"mnist/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); In [ ]: ! pip install elegy dataget In [ ]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () In [2]: import jax.numpy as jnp import jax import haiku as hk def model_fn ( image ) -> jnp . ndarray : \"\"\"Standard LeNet-300-100 MLP network.\"\"\" image = image . astype ( jnp . float32 ) / 255.0 mlp = hk . Sequential ( [ hk . Flatten (), hk . Linear ( 300 ), jax . nn . relu , hk . Linear ( 100 ), jax . nn . relu , hk . Linear ( 10 ), ] ) return mlp ( image ) In [ ]: import elegy model = elegy . Model ( model_fn = model_fn , loss = lambda : elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), aux_losses = lambda : elegy . regularizers . GlobalL2Regularization ( l = 1e-4 ), metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , batch_size = 64 , steps_per_epoch = 100 , validation_data = ( X_test , y_test ), shuffle = True , ) In [5]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) # plt.ylim([min(plt.ylim()), 1]) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history ) In [14]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) y_pred = model . predict ( x = X_test [ idxs ]) plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( X_test [ idxs [ k ]], cmap = \"gray\" )","title":"Getting Started"},{"location":"api/losses/CategoricalCrossentropy/","text":"elegy.losses.CategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy () assert cce ( y_true , y_pred ) == 1.177 # Calling with 'sample_weight'. assert cce ( y_true , y_pred , sample_weight = tf . constant ([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) assert cce ( y_true , y_pred ) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) assert list ( cce ( y_true , y_pred )) == [ 0.0513 , 2.303 ] Usage with the compile API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , name = None , weight = None ) special Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None name Optional[str] Optional name for the op. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/categorical_crossentropy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing call ( self , y_true , y_pred , sample_weight = None ) Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegylossescategoricalcrossentropy","text":"","title":"elegy.losses.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy () assert cce ( y_true , y_pred ) == 1.177 # Calling with 'sample_weight'. assert cce ( y_true , y_pred , sample_weight = tf . constant ([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) assert cce ( y_true , y_pred ) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) assert list ( cce ( y_true , y_pred )) == [ 0.0513 , 2.303 ] Usage with the compile API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), )","title":"elegy.losses.categorical_crossentropy.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.__init__","text":"Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None name Optional[str] Optional name for the op. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/categorical_crossentropy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.call","text":"Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"call()"},{"location":"api/losses/MeanSquaredError/","text":"elegy.losses.MeanSquaredError Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError () assert mse ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mse ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . SUM ) assert mse ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mse ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . MeanSquaredError ()], metrics = lambda : [ elegy . metrics . Mean ()], ) __init__ ( self , reduction = None , name = None , weight = None ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None name Optional[str] Optional name for the loss. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/mean_squared_error.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. name: Optional name for the loss. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , name = name , weight = weight ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegylossesmeansquarederror","text":"","title":"elegy.losses.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError","text":"Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array ([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array ([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError () assert mse ( y_true , y_pred ) == 0.5 # Calling with 'sample_weight'. assert mse ( y_true , y_pred , sample_weight = jnp . array ([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . SUM ) assert mse ( y_true , y_pred ) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError ( reduction = elegy . losses . Reduction . NONE ) assert list ( mse ( y_true , y_pred )) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . MeanSquaredError ()], metrics = lambda : [ elegy . metrics . Mean ()], )","title":"elegy.losses.mean_squared_error.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None name Optional[str] Optional name for the loss. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/mean_squared_error.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. name: Optional name for the loss. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" return super () . __init__ ( reduction = reduction , name = name , weight = weight )","title":"__init__()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/SparseCategoricalCrossentropy/","text":"elegy.losses.SparseCategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array ([ 1 , 2 ]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy () result = scce ( y_true , y_pred ) # 1.177 assert jnp . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( y_true , y_pred , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = scce ( y_true , y_pred ) # 2.354 assert jnp . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = scce ( y_true , y_pred ) # [0.0513, 2.303] assert jnp . all ( jnp . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the compile API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , name = None , weight = None ) special Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. None name Optional[str] Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/sparse_categorical_crossentropy.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing call ( self , y_true , y_pred , sample_weight = None ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegylossessparsecategoricalcrossentropy","text":"","title":"elegy.losses.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array ([ 1 , 2 ]) y_pred = jnp . array ([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy () result = scce ( y_true , y_pred ) # 1.177 assert jnp . isclose ( result , 1.177 , rtol = 0.01 ) # Calling with 'sample_weight'. result = scce ( y_true , y_pred , sample_weight = jnp . array ([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose ( result , 0.814 , rtol = 0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . SUM ) result = scce ( y_true , y_pred ) # 2.354 assert jnp . isclose ( result , 2.354 , rtol = 0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy ( reduction = elegy . losses . Reduction . NONE ) result = scce ( y_true , y_pred ) # [0.0513, 2.303] assert jnp . all ( jnp . isclose ( result , [ 0.0513 , 2.303 ], rtol = 0.01 )) Usage with the compile API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), )","title":"elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. None name Optional[str] Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None Source code in elegy/losses/sparse_categorical_crossentropy.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , name : tp . Optional [ str ] = None , weight : tp . Optional [ float ] = None , ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. name: Optional name for the op. Defaults to 'sparse_categorical_crossentropy'. weight: Optional weight contribution for the total loss. Defaults to `1`. \"\"\" super () . __init__ ( reduction = reduction , name = name , weight = weight ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits )","title":"call()"},{"location":"api/losses/mean_squared_error/","text":"elegy.losses.mean_squared_error Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegylossesmean_squared_error","text":"","title":"elegy.losses.mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegy.losses.mean_squared_error.mean_squared_error","text":"Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean ( square ( y_true - y_pred ), axis =- 1 ) Usage: rng = jax . random . PRNGKey ( 42 ) y_true = jax . random . randint ( rng , shape = ( 2 , 3 ), minval = 0 , maxval = 2 ) y_pred = jax . random . uniform ( rng , shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error ( y_true , y_pred ) assert loss . shape == ( 2 ,) assert jnp . array_equal ( loss , jnp . mean ( jnp . square ( y_true - y_pred ), axis =- 1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_squared_error.mean_squared_error"},{"location":"api/metrics/Accuracy/","text":"elegy.metrics.Accuracy Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy () result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , name = None , dtype = None ) special Creates a Accuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/accuracy.py 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `Accuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"Accuracy"},{"location":"api/metrics/Accuracy/#elegymetricsaccuracy","text":"","title":"elegy.metrics.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy","text":"Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy () result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . Accuracy ()], optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.accuracy.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.__init__","text":"Creates a Accuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/accuracy.py 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `Accuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/CategoricalAccuracy/","text":"elegy.metrics.CategoricalAccuracy Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy () result = accuracy ( y_true = jnp . array ([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . CategoricalAccuracy ()], optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , name = None , dtype = None ) special Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/categorical_accuracy.py 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegymetricscategoricalaccuracy","text":"","title":"elegy.metrics.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy","text":"Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy () result = accuracy ( y_true = jnp . array ([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . CategoricalAccuracy ()], optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.categorical_accuracy.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.__init__","text":"Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/categorical_accuracy.py 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/Mean/","text":"elegy.metrics.Mean Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . MeanSquaredError ()], metrics = lambda : [ elegy . metrics . Mean ()], ) __init__ ( self , name = None , dtype = None ) special Creates a Mean instance. Parameters: Name Type Description Default name Optional[str] (Optional) string name of the metric instance. None dtype Optional[numpy.dtype] (Optional) data type of the metric result. None Source code in elegy/metrics/mean.py 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. \"\"\" super () . __init__ ( reduction = reduce . Reduction . WEIGHTED_MEAN , name = name , dtype = dtype ) call ( self , values , sample_weight = None ) Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"Mean"},{"location":"api/metrics/Mean/#elegymetricsmean","text":"","title":"elegy.metrics.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean","text":"Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean () result = mean ([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean ([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . MeanSquaredError ()], metrics = lambda : [ elegy . metrics . Mean ()], )","title":"elegy.metrics.mean.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.__init__","text":"Creates a Mean instance. Parameters: Name Type Description Default name Optional[str] (Optional) string name of the metric instance. None dtype Optional[numpy.dtype] (Optional) data type of the metric result. None Source code in elegy/metrics/mean.py 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\"Creates a `Mean` instance. Arguments: name: (Optional) string name of the metric instance. dtype: (Optional) data type of the metric result. \"\"\" super () . __init__ ( reduction = reduce . Reduction . WEIGHTED_MEAN , name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.call","text":"Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"call()"},{"location":"api/metrics/MeanSquaredError/","text":"elegy.metrics.MeanSquaredError Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError () result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . MeanSquaredError ()], ) __init__ ( self , name = None , dtype = None ) special Creates a MeanSquaredError instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/mean_squared_error.py 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegymetricsmeansquarederror","text":"","title":"elegy.metrics.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError","text":"Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError () result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse ( y_true = jnp . array ([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array ([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . MeanSquaredError ()], )","title":"elegy.metrics.mean_squared_error.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.__init__","text":"Creates a MeanSquaredError instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/mean_squared_error.py 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"call()"},{"location":"api/metrics/SparseCategoricalAccuracy/","text":"elegy.metrics.SparseCategoricalAccuracy Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy () result = accuracy ( y_true = jnp . array ([ 2 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([ 1 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . SparseCategoricalAccuracy ()], optimizer = optix . adam ( 1e-3 ), ) __init__ ( self , name = None , dtype = None ) special Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/sparse_categorical_accuracy.py 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegymetricssparsecategoricalaccuracy","text":"","title":"elegy.metrics.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy","text":"Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy () result = accuracy ( y_true = jnp . array ([ 2 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy ( y_true = jnp . array ([ 1 , 1 ]), y_pred = jnp . array ([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . CategoricalCrossentropy ()], metrics = lambda : [ elegy . metrics . SparseCategoricalAccuracy ()], optimizer = optix . adam ( 1e-3 ), )","title":"elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.__init__","text":"Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default name Optional[str] string name of the metric instance. None dtype Optional[numpy.dtype] data type of the metric result. None Source code in elegy/metrics/sparse_categorical_accuracy.py 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Optional [ jnp . dtype ] = None , ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: name: string name of the metric instance. dtype: data type of the metric result. \"\"\" super () . __init__ ( name = name , dtype = dtype )","title":"__init__()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/regularizers/GlobalL1L2Regularization/","text":"elegy.regularizers.GlobalL1L2Regularization A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( model_fn = model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . regularizers . GlobalL1L2Regularization ( l1 = 1e-5 , l2 = 1e-4 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor. call ( self , params ) Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default params Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]] A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2_regularization.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def call ( self , params : hk . Params ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( params ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( params ) ) return regularization","title":"GlobalL1L2Regularization"},{"location":"api/regularizers/GlobalL1L2Regularization/#elegyregularizersgloball1l2regularization","text":"","title":"elegy.regularizers.GlobalL1L2Regularization"},{"location":"api/regularizers/GlobalL1L2Regularization/#elegy.regularizers.global_l1l2_regularization.GlobalL1L2Regularization","text":"A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( model_fn = model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . regularizers . GlobalL1L2Regularization ( l1 = 1e-5 , l2 = 1e-4 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor.","title":"elegy.regularizers.global_l1l2_regularization.GlobalL1L2Regularization"},{"location":"api/regularizers/GlobalL1L2Regularization/#elegy.regularizers.global_l1l2_regularization.GlobalL1L2Regularization.call","text":"Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default params Mapping[str, Mapping[str, jax.numpy.lax_numpy.ndarray]] A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2_regularization.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def call ( self , params : hk . Params ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: params: A structure with all the parameters of the model. \"\"\" regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( params ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( params ) ) return regularization","title":"call()"},{"location":"api/regularizers/GlobalL1Regularization/","text":"elegy.regularizers.GlobalL1Regularization Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model ( model_fn = model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . regularizers . GlobalL1Regularization ( l = 1e-5 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 Returns: Type Description GlobalL1L2Regularization An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1_regularization.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def GlobalL1Regularization ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ) -> GlobalL1L2Regularization : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( model_fn=model_fn, loss=lambda: [elegy.losses.SparseCategoricalCrossentropy()], aux_losses=lambda: [elegy.regularizers.GlobalL1Regularization(l=1e-5)], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2Regularization ( l1 = l , reduction = reduction , name = name )","title":"GlobalL1Regularization"},{"location":"api/regularizers/GlobalL1Regularization/#elegyregularizersgloball1regularization","text":"","title":"elegy.regularizers.GlobalL1Regularization"},{"location":"api/regularizers/GlobalL1Regularization/#elegy.regularizers.global_l1_regularization.GlobalL1Regularization","text":"Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model ( model_fn = model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . regularizers . GlobalL1Regularization ( l = 1e-5 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 Returns: Type Description GlobalL1L2Regularization An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1_regularization.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def GlobalL1Regularization ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ) -> GlobalL1L2Regularization : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( model_fn=model_fn, loss=lambda: [elegy.losses.SparseCategoricalCrossentropy()], aux_losses=lambda: [elegy.regularizers.GlobalL1Regularization(l=1e-5)], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2Regularization ( l1 = l , reduction = reduction , name = name )","title":"elegy.regularizers.global_l1_regularization.GlobalL1Regularization"},{"location":"api/regularizers/GlobalL2Regularization/","text":"elegy.regularizers.GlobalL2Regularization Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . losses . GlobaL2Regularization ( l = 1e-4 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2Regularization An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2_regularization.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def GlobalL2Regularization ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2Regularization : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( model_fn, loss=lambda: [elegy.losses.SparseCategoricalCrossentropy()], aux_losses=lambda: [elegy.losses.GlobaL2Regularization(l=1e-4)], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2Regularization ( l2 = l , reduction = reduction , name = name )","title":"GlobalL2Regularization"},{"location":"api/regularizers/GlobalL2Regularization/#elegyregularizersgloball2regularization","text":"","title":"elegy.regularizers.GlobalL2Regularization"},{"location":"api/regularizers/GlobalL2Regularization/#elegy.regularizers.global_l2_regularization.GlobalL2Regularization","text":"Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model ( model_fn , loss = lambda : [ elegy . losses . SparseCategoricalCrossentropy ()], aux_losses = lambda : [ elegy . losses . GlobaL2Regularization ( l = 1e-4 )], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy (), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2Regularization An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2_regularization.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def GlobalL2Regularization ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2Regularization : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( model_fn, loss=lambda: [elegy.losses.SparseCategoricalCrossentropy()], aux_losses=lambda: [elegy.losses.GlobaL2Regularization(l=1e-4)], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2Regularization ( l2 = l , reduction = reduction , name = name )","title":"elegy.regularizers.global_l2_regularization.GlobalL2Regularization"}]}